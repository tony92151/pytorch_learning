{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import GPUtil\n",
    "GPUtil.showUtilization()\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIGNSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A standard PyTorch definition of Dataset which defines the functions __len__ and __getitem__.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, transform):\n",
    "        \"\"\"\n",
    "        Store the filenames of the jpgs to use. Specifies transforms to apply on images.\n",
    "        Args:\n",
    "            data_dir: (string) directory containing the dataset\n",
    "            transform: (torchvision.transforms) transformation to apply on image\n",
    "        \"\"\"\n",
    "        self.filenames = os.listdir(data_dir)\n",
    "        self.filenames = [os.path.join(data_dir, f) for f in self.filenames if f.endswith('.jpg')]\n",
    "\n",
    "        self.labels = [int(os.path.split(filename)[-1][0]) for filename in self.filenames]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return size of dataset\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch index idx image and labels from dataset. Perform transforms on image.\n",
    "        Args:\n",
    "            idx: (int) index in [0, 1, ..., size_of_dataset-1]\n",
    "        Returns:\n",
    "            image: (Tensor) transformed image\n",
    "            label: (int) corresponding label of image\n",
    "        \"\"\"\n",
    "        image = Image.open(self.filenames[idx])  # PIL image\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "\n",
    "train_data_path = \"dataset/train_signs/\"\n",
    "traindata = SIGNSDataset(train_data_path, train_transformer)\n",
    "\n",
    "dl = DataLoader(traindata, batch_size=32, shuffle=True,\n",
    "                                        num_workers=4,\n",
    "                                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformer = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "\n",
    "test_data_path = \"dataset/test_signs/\"\n",
    "testdata = SIGNSDataset(train_data_path, train_transformer)\n",
    "\n",
    "tl = DataLoader(testdata, batch_size=32, shuffle=False,\n",
    "                                        num_workers=4,\n",
    "                                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "a,b = dl.dataset[0]\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard way to define your own network in PyTorch. You typically choose the components\n",
    "    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n",
    "    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n",
    "    such as F.relu, F.sigmoid, F.softmax, F.max_pool2d. Be careful to ensure your dimensions are correct after each\n",
    "    step. You are encouraged to have a look at the network in pytorch/nlp/model/net.py to get a better sense of how\n",
    "    you can go about defining your own network.\n",
    "    The documentation for all the various components available o you is here: http://pytorch.org/docs/master/nn.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        self.num_channels = 32\n",
    "        \n",
    "        # each of the convolution layers below have the arguments (input_channels, output_channels, filter_size,\n",
    "        # stride, padding). We also include batch normalisation layers that help stabilise training.\n",
    "        # For more details on how to use these layers, check out the documentation.\n",
    "        self.conv1 = nn.Conv2d(3, self.num_channels, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.num_channels)\n",
    "        self.conv2 = nn.Conv2d(self.num_channels, self.num_channels*2, 3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.num_channels*2)\n",
    "        self.conv3 = nn.Conv2d(self.num_channels*2, self.num_channels*4, 3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.num_channels*4)\n",
    "\n",
    "        # 2 fully connected layers to transform the output of the convolution layers to the final output\n",
    "        self.fc1 = nn.Linear(8*8*self.num_channels*4, self.num_channels*4)\n",
    "        self.fcbn1 = nn.BatchNorm1d(self.num_channels*4)\n",
    "        self.fc2 = nn.Linear(self.num_channels*4, 6)       \n",
    "        self.dropout_rate = 0.8\n",
    "\n",
    "    def forward(self, s):\n",
    "\n",
    "        #                                                  -> batch_size x 3 x 64 x 64\n",
    "        # we apply the convolution layers, followed by batch normalisation, maxpool and relu x 3\n",
    "        s = self.bn1(self.conv1(s))                         # batch_size x num_channels x 64 x 64\n",
    "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels x 32 x 32\n",
    "        s = self.bn2(self.conv2(s))                         # batch_size x num_channels*2 x 32 x 32\n",
    "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*2 x 16 x 16\n",
    "        s = self.bn3(self.conv3(s))                         # batch_size x num_channels*4 x 16 x 16\n",
    "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*4 x 8 x 8\n",
    "\n",
    "        # flatten the output for each image\n",
    "        s = s.view(-1, 8*8*self.num_channels*4)             # batch_size x 8*8*num_channels*4\n",
    "\n",
    "        # apply 2 fully connected layers with dropout\n",
    "        s = F.dropout(F.relu(self.fcbn1(self.fc1(s))), \n",
    "            p=self.dropout_rate, training=self.training)    # batch_size x self.num_channels*4\n",
    "        s = self.fc2(s)                                     # batch_size x 6\n",
    "\n",
    "        # apply log softmax on each image's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=8192, out_features=128, bias=True)\n",
      "  (fcbn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cnn = Net().cuda()\n",
    "    print(\"Run in GPU\")\n",
    "else:\n",
    "    cnn = Net()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for i, (train_batch, labels_batch) in enumerate(dl):\n",
    "        train_batch, labels_batch = train_batch, labels_batch\n",
    "        # convert to torch Variables\n",
    "        train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = cnn(train_batch)\n",
    "        loss = loss_func(output_batch, labels_batch)\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "        if i % 8 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                    i * len(train_batch), len(dl.dataset),100. * i / len(dl), loss.data.item()))\n",
    "    print(\"Finish\")\n",
    "    GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    cnn.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in tl:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = cnn(data)\n",
    "        # sum up batch loss\n",
    "        los = loss_func(output, target)\n",
    "        #print(los)\n",
    "        test_loss += los\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    #print(test_loss)\n",
    "    test_loss /= len(tl)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(tl.dataset),\n",
    "        100. * correct / len(tl.dataset)))\n",
    "    \n",
    "    \n",
    "    #print(float(test_loss))\n",
    "    global Loss\n",
    "    global Accuracy\n",
    "    #Loss = np.append(Loss,float(test_loss))\n",
    "    #print(Loss)\n",
    "    #Accuracy = np.append(Accuracy,float(100. * correct / len(tl.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/864 (0%)]\tLoss: 1.902371\n",
      "Train Epoch: 1 [256/864 (30%)]\tLoss: 1.487707\n",
      "Train Epoch: 1 [512/864 (59%)]\tLoss: 1.558318\n",
      "Train Epoch: 1 [768/864 (89%)]\tLoss: 1.292510\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 2.0985, Accuracy: 160/864 (18%)\n",
      "\n",
      "Train Epoch: 2 [0/864 (0%)]\tLoss: 1.998375\n",
      "Train Epoch: 2 [256/864 (30%)]\tLoss: 1.610687\n",
      "Train Epoch: 2 [512/864 (59%)]\tLoss: 1.420069\n",
      "Train Epoch: 2 [768/864 (89%)]\tLoss: 1.545844\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 1.1183, Accuracy: 469/864 (54%)\n",
      "\n",
      "Train Epoch: 3 [0/864 (0%)]\tLoss: 1.084097\n",
      "Train Epoch: 3 [256/864 (30%)]\tLoss: 0.995580\n",
      "Train Epoch: 3 [512/864 (59%)]\tLoss: 0.798327\n",
      "Train Epoch: 3 [768/864 (89%)]\tLoss: 0.643231\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.6215, Accuracy: 687/864 (79%)\n",
      "\n",
      "Train Epoch: 4 [0/864 (0%)]\tLoss: 0.559471\n",
      "Train Epoch: 4 [256/864 (30%)]\tLoss: 0.448374\n",
      "Train Epoch: 4 [512/864 (59%)]\tLoss: 0.757775\n",
      "Train Epoch: 4 [768/864 (89%)]\tLoss: 0.768452\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.4385, Accuracy: 744/864 (86%)\n",
      "\n",
      "Train Epoch: 5 [0/864 (0%)]\tLoss: 0.478369\n",
      "Train Epoch: 5 [256/864 (30%)]\tLoss: 0.885602\n",
      "Train Epoch: 5 [512/864 (59%)]\tLoss: 0.393175\n",
      "Train Epoch: 5 [768/864 (89%)]\tLoss: 0.438060\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.5406, Accuracy: 685/864 (79%)\n",
      "\n",
      "Train Epoch: 6 [0/864 (0%)]\tLoss: 0.676565\n",
      "Train Epoch: 6 [256/864 (30%)]\tLoss: 0.251982\n",
      "Train Epoch: 6 [512/864 (59%)]\tLoss: 0.447744\n",
      "Train Epoch: 6 [768/864 (89%)]\tLoss: 0.365370\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.2298, Accuracy: 801/864 (92%)\n",
      "\n",
      "Train Epoch: 7 [0/864 (0%)]\tLoss: 0.186718\n",
      "Train Epoch: 7 [256/864 (30%)]\tLoss: 0.276356\n",
      "Train Epoch: 7 [512/864 (59%)]\tLoss: 0.176105\n",
      "Train Epoch: 7 [768/864 (89%)]\tLoss: 0.303837\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.2832, Accuracy: 781/864 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/864 (0%)]\tLoss: 0.137042\n",
      "Train Epoch: 8 [256/864 (30%)]\tLoss: 0.087178\n",
      "Train Epoch: 8 [512/864 (59%)]\tLoss: 0.064740\n",
      "Train Epoch: 8 [768/864 (89%)]\tLoss: 0.215282\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.2747, Accuracy: 787/864 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/864 (0%)]\tLoss: 0.220482\n",
      "Train Epoch: 9 [256/864 (30%)]\tLoss: 0.239208\n",
      "Train Epoch: 9 [512/864 (59%)]\tLoss: 0.346352\n",
      "Train Epoch: 9 [768/864 (89%)]\tLoss: 0.204832\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.1222, Accuracy: 836/864 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/864 (0%)]\tLoss: 0.214143\n",
      "Train Epoch: 10 [256/864 (30%)]\tLoss: 0.040727\n",
      "Train Epoch: 10 [512/864 (59%)]\tLoss: 0.110678\n",
      "Train Epoch: 10 [768/864 (89%)]\tLoss: 0.053054\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.0714, Accuracy: 846/864 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/864 (0%)]\tLoss: 0.027890\n",
      "Train Epoch: 11 [256/864 (30%)]\tLoss: 0.042797\n",
      "Train Epoch: 11 [512/864 (59%)]\tLoss: 0.309485\n",
      "Train Epoch: 11 [768/864 (89%)]\tLoss: 0.007594\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.0943, Accuracy: 843/864 (97%)\n",
      "\n",
      "Train Epoch: 12 [0/864 (0%)]\tLoss: 0.132468\n",
      "Train Epoch: 12 [256/864 (30%)]\tLoss: 0.077066\n",
      "Train Epoch: 12 [512/864 (59%)]\tLoss: 0.219787\n",
      "Train Epoch: 12 [768/864 (89%)]\tLoss: 0.085283\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.0525, Accuracy: 853/864 (98%)\n",
      "\n",
      "Train Epoch: 13 [0/864 (0%)]\tLoss: 0.043064\n",
      "Train Epoch: 13 [256/864 (30%)]\tLoss: 0.015654\n",
      "Train Epoch: 13 [512/864 (59%)]\tLoss: 0.028659\n",
      "Train Epoch: 13 [768/864 (89%)]\tLoss: 0.120002\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.0261, Accuracy: 861/864 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/864 (0%)]\tLoss: 0.017104\n",
      "Train Epoch: 14 [256/864 (30%)]\tLoss: 0.068904\n",
      "Train Epoch: 14 [512/864 (59%)]\tLoss: 0.095356\n",
      "Train Epoch: 14 [768/864 (89%)]\tLoss: 0.003097\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.0202, Accuracy: 859/864 (99%)\n",
      "\n",
      "Train Epoch: 15 [0/864 (0%)]\tLoss: 0.005369\n",
      "Train Epoch: 15 [256/864 (30%)]\tLoss: 0.021892\n",
      "Train Epoch: 15 [512/864 (59%)]\tLoss: 0.374314\n",
      "Train Epoch: 15 [768/864 (89%)]\tLoss: 0.142784\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.4385, Accuracy: 722/864 (83%)\n",
      "\n",
      "Train Epoch: 16 [0/864 (0%)]\tLoss: 0.630366\n",
      "Train Epoch: 16 [256/864 (30%)]\tLoss: 0.111167\n",
      "Train Epoch: 16 [512/864 (59%)]\tLoss: 0.629690\n",
      "Train Epoch: 16 [768/864 (89%)]\tLoss: 0.071107\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.1129, Accuracy: 844/864 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/864 (0%)]\tLoss: 0.113275\n",
      "Train Epoch: 17 [256/864 (30%)]\tLoss: 0.078454\n",
      "Train Epoch: 17 [512/864 (59%)]\tLoss: 0.072954\n",
      "Train Epoch: 17 [768/864 (89%)]\tLoss: 0.095685\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.0502, Accuracy: 848/864 (98%)\n",
      "\n",
      "Train Epoch: 18 [0/864 (0%)]\tLoss: 0.025381\n",
      "Train Epoch: 18 [256/864 (30%)]\tLoss: 0.052202\n",
      "Train Epoch: 18 [512/864 (59%)]\tLoss: 0.049406\n",
      "Train Epoch: 18 [768/864 (89%)]\tLoss: 0.132199\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.0522, Accuracy: 852/864 (98%)\n",
      "\n",
      "Train Epoch: 19 [0/864 (0%)]\tLoss: 0.020259\n",
      "Train Epoch: 19 [256/864 (30%)]\tLoss: 0.014735\n",
      "Train Epoch: 19 [512/864 (59%)]\tLoss: 0.081304\n",
      "Train Epoch: 19 [768/864 (89%)]\tLoss: 0.006228\n",
      "Finish\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "\n",
      "Test set: Average loss: 0.0238, Accuracy: 862/864 (99%)\n",
      "\n",
      "Total cost 219.607721 sec\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,20):\n",
    "    if epoch == 1:\n",
    "        ts = time.time()\n",
    "    train(epoch)\n",
    "    test()\n",
    "te = time.time()\n",
    "print(\"Total cost %f sec\" % (te - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonyguo/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(cnn, 'cnn_stanford2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single image test\n",
    "import matplotlib.pyplot as plt\n",
    "single_transforms = transforms.Compose(\n",
    "        [\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "        ]\n",
    ")\n",
    "model  = torch.load('cnn_stanford2.pkl').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "torch.Size([1, 3, 64, 64])\n",
      "(3, 64, 64)\n",
      "(64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.open(\"image.jpg\")\n",
    "image = single_transforms(img).cpu()\n",
    "imgToShow = image.numpy()\n",
    "print(image.shape)\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)\n",
    "\n",
    "print(imgToShow.shape)\n",
    "imgToShow = imgToShow.transpose((1, 2, 0))\n",
    "print(imgToShow.shape)\n",
    "plt.imshow(imgToShow, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "single_out = model(image)\n",
    "#print(single_out)\n",
    "p = torch.max(single_out, 1)[1].data.numpy().squeeze()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onnx to caffe2 \n",
    "batch_size=1  # 随便一个数\n",
    "x = Variable(torch.randn(batch_size,3,64,64), requires_grad=True).cuda()\n",
    "torch_out = torch.onnx._export(cnn, x, \"onnx/cnn_stanford.onnx\", export_params=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph torch-jit-export (\\n  %0[FLOAT, 1x3x64x64]\\n) initializers (\\n  %1[FLOAT, 32x3x3x3]\\n  %2[FLOAT, 32]\\n  %3[FLOAT, 32]\\n  %4[FLOAT, 32]\\n  %5[FLOAT, 32]\\n  %6[FLOAT, 32]\\n  %7[INT64, scalar]\\n  %8[FLOAT, 64x32x3x3]\\n  %9[FLOAT, 64]\\n  %10[FLOAT, 64]\\n  %11[FLOAT, 64]\\n  %12[FLOAT, 64]\\n  %13[FLOAT, 64]\\n  %14[INT64, scalar]\\n  %15[FLOAT, 128x64x3x3]\\n  %16[FLOAT, 128]\\n  %17[FLOAT, 128]\\n  %18[FLOAT, 128]\\n  %19[FLOAT, 128]\\n  %20[FLOAT, 128]\\n  %21[INT64, scalar]\\n  %22[FLOAT, 128x8192]\\n  %23[FLOAT, 128]\\n  %24[FLOAT, 128]\\n  %25[FLOAT, 128]\\n  %26[FLOAT, 128]\\n  %27[FLOAT, 128]\\n  %28[INT64, scalar]\\n  %29[FLOAT, 6x128]\\n  %30[FLOAT, 6]\\n) {\\n  %31 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%0, %1, %2)\\n  %32 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%31, %3, %4, %5, %6)\\n  %33 = MaxPool[kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%32)\\n  %34 = Relu(%33)\\n  %35 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%34, %8, %9)\\n  %36 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%35, %10, %11, %12, %13)\\n  %37 = MaxPool[kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%36)\\n  %38 = Relu(%37)\\n  %39 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%38, %15, %16)\\n  %40 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%39, %17, %18, %19, %20)\\n  %41 = MaxPool[kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%40)\\n  %42 = Relu(%41)\\n  %43 = Constant[value = <Tensor>]()\\n  %44 = Reshape(%42, %43)\\n  %45 = Gemm[alpha = 1, beta = 1, transB = 1](%44, %22, %23)\\n  %46 = Unsqueeze[axes = [2]](%45)\\n  %47 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%46, %24, %25, %26, %27)\\n  %48 = Squeeze[axes = [2]](%47)\\n  %49 = Relu(%48)\\n  %50, %51 = Dropout[ratio = 0.800000011920929](%49)\\n  %52 = Gemm[alpha = 1, beta = 1, transB = 1](%50, %29, %30)\\n  %53 = LogSoftmax[axis = 1](%52)\\n  return %53\\n}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"onnx/cnn_stanford.onnx\")\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "# Print a human readable representation of the graph\n",
    "onnx.helper.printable_graph(model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: 0 type: float64\n"
     ]
    }
   ],
   "source": [
    "# ...continuing from above\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "outputs = rep.run(np.random.randn(10, 3, 64, 64).astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "\n",
    "from caffe2.python.onnx.backend import Caffe2Backend as c2\n",
    "init_net, predict_net = c2.onnx_graph_to_caffe2_net(model)\n",
    "with open(\"onnx/init_net.pb\", \"wb\") as f:\n",
    "    f.write(init_net.SerializeToString())\n",
    "with open(\"onnx/predict_net.pb\", \"wb\") as f:\n",
    "    f.write(predict_net.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
